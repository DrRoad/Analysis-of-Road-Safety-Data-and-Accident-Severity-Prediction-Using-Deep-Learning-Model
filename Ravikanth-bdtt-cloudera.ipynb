{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load / Ingest the data using sqoop and hive in Clourera VM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Check the list of tables in MySQL database with sqoop list-tables command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/04/21 10:08:48 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5-cdh5.4.3\n",
      "19/04/21 10:08:48 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.\n",
      "19/04/21 10:08:49 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.\n",
      "accident_severity\n",
      "accidents\n",
      "casualties\n"
     ]
    }
   ],
   "source": [
    "#Listing all tables in db_bdtt_ac database\n",
    "! sqoop list-tables \\\n",
    "--connect jdbc:mysql://localhost/db_bdtt_ac --username training --password training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Import Accidents table from MySQL to HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I have imported accidents table from MySQL to hadoop file system by using credentials from database.props file.I have used warehouse directory to create a folder with table name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/04/21 08:29:24 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5-cdh5.4.3\n",
      "19/04/21 08:29:24 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.\n",
      "19/04/21 08:29:25 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.\n",
      "19/04/21 08:29:25 INFO tool.CodeGenTool: Beginning code generation\n",
      "19/04/21 08:29:25 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `accidents` AS t LIMIT 1\n",
      "19/04/21 08:29:25 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `accidents` AS t LIMIT 1\n",
      "19/04/21 08:29:25 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce\n",
      "Note: /tmp/sqoop-training/compile/9b8fcf75d6abd7288fe06622c4c2645b/accidents.java uses or overrides a deprecated API.\n",
      "Note: Recompile with -Xlint:deprecation for details.\n",
      "19/04/21 08:29:29 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-training/compile/9b8fcf75d6abd7288fe06622c4c2645b/accidents.jar\n",
      "19/04/21 08:29:29 WARN manager.MySQLManager: It looks like you are importing from mysql.\n",
      "19/04/21 08:29:29 WARN manager.MySQLManager: This transfer can be faster! Use the --direct\n",
      "19/04/21 08:29:29 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.\n",
      "19/04/21 08:29:29 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)\n",
      "19/04/21 08:29:29 INFO mapreduce.ImportJobBase: Beginning import of accidents\n",
      "19/04/21 08:29:29 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "19/04/21 08:29:29 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n",
      "19/04/21 08:29:30 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "19/04/21 08:29:30 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/04/21 08:29:32 INFO db.DBInputFormat: Using read commited transaction isolation\n",
      "19/04/21 08:29:32 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "19/04/21 08:29:32 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1555436240145_0002\n",
      "19/04/21 08:29:33 INFO impl.YarnClientImpl: Submitted application application_1555436240145_0002\n",
      "19/04/21 08:29:33 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1555436240145_0002/\n",
      "19/04/21 08:29:33 INFO mapreduce.Job: Running job: job_1555436240145_0002\n",
      "19/04/21 08:29:41 INFO mapreduce.Job: Job job_1555436240145_0002 running in uber mode : false\n",
      "19/04/21 08:29:41 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/04/21 08:29:55 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/04/21 08:29:55 INFO mapreduce.Job: Job job_1555436240145_0002 completed successfully\n",
      "19/04/21 08:29:56 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=136967\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=87\n",
      "\t\tHDFS: Number of bytes written=19004812\n",
      "\t\tHDFS: Number of read operations=4\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tOther local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=11778\n",
      "\t\tTotal vcore-seconds taken by all map tasks=11778\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=3015168\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=129982\n",
      "\t\tMap output records=129982\n",
      "\t\tInput split bytes=87\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=160\n",
      "\t\tCPU time spent (ms)=3520\n",
      "\t\tPhysical memory (bytes) snapshot=119115776\n",
      "\t\tVirtual memory (bytes) snapshot=845750272\n",
      "\t\tTotal committed heap usage (bytes)=47972352\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=19004812\n",
      "19/04/21 08:29:56 INFO mapreduce.ImportJobBase: Transferred 18.1244 MB in 25.5933 seconds (725.1657 KB/sec)\n",
      "19/04/21 08:29:56 INFO mapreduce.ImportJobBase: Retrieved 129982 records.\n"
     ]
    }
   ],
   "source": [
    "! sqoop --options-file ./database.props \\\n",
    "        --table accidents \\\n",
    "        --split-by 'Accident_Index'\\\n",
    "        --warehouse-dir /db_bdtt_ac \\\n",
    "        --null-string '-1' \\\n",
    "        --null-non-string '-1' \\\n",
    "        --m 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Check the warehouse directory for import status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-rw-rw-   1 training supergroup          0 2019-04-21 08:29 /db_bdtt_ac/accidents/_SUCCESS\r\n",
      "-rw-rw-rw-   1 training supergroup   19004812 2019-04-21 08:29 /db_bdtt_ac/accidents/part-m-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /db_bdtt_ac/accidents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####1.2.2. Cheking imported data with default number of mappers in warehouse directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017010001708,532920.0,196330.0,-0.08010700,51.65006100,1,1,2,3,05/08/2017,7,03:12,32,E09000010,3,105,6,30.0,0,-1,-1,0,0,0,4,1,1,0,0,1,1,E01001450\r\n",
      "2017010009342,526790.0,181970.0,-0.17384500,51.52242500,1,3,2,1,01/01/2017,1,01:30,1,E09000033,3,5,6,30.0,3,4,6,0,0,0,4,1,2,0,0,1,1,E01004702\r\n",
      "2017010009344,535200.0,181260.0,-0.05296900,51.51409600,1,3,3,1,01/01/2017,1,00:30,5,E09000030,3,13,6,30.0,3,4,5,0,0,0,4,1,1,0,0,1,1,E01004298\r\n",
      "2017010009348,534340.0,193560.0,-0.06065800,51.62483200,1,3,2,1,01/01/2017,1,01:11,32,E09000010,3,1010,1,30.0,1,4,4,154,0,4,4,2,2,0,0,1,1,E01001429\r\n",
      "2017010009350,533680.0,187820.0,-0.07237200,51.57340800,1,2,1,1,01/01/2017,1,01:42,4,E09000012,3,107,3,20.0,6,2,3,10,0,5,4,1,2,0,0,1,1,E01001808\r\n",
      "2017010009351,514510.0,172370.0,-0.35387600,51.43876200,1,3,2,1,01/01/2017,1,03:31,24,E09000027,6,0,6,30.0,0,-1,-1,0,0,0,4,1,2,0,0,1,1,E01003900\r\n",
      "2017010009353,508640.0,181870.0,-0.43537700,51.52530500,1,3,2,2,01/01/2017,1,04:07,26,E09000017,3,4020,3,40.0,3,4,5,0,0,4,4,1,2,0,0,1,1,E01002401\r\n",
      "2017010009354,527880.0,181950.0,-0.15815000,51.52200000,1,3,2,1,01/01/2017,1,05:20,1,E09000033,3,501,3,30.0,0,-1,-1,0,2,5,4,2,2,4,0,1,1,E01004660\r\n",
      "2017010009357,520940.0,192820.0,-0.25439300,51.62121900,1,2,1,1,01/01/2017,1,03:18,30,E09000003,3,1,3,50.0,1,2,3,5109,0,4,4,2,2,0,0,1,1,E01000264\r\n",
      "2017010009358,531430.0,178450.0,-0.10831400,51.48973200,1,2,1,1,01/01/2017,1,03:00,9,E09000022,3,3204,6,30.0,3,4,6,0,0,0,4,1,2,0,0,1,1,E01003106\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -cat /db_bdtt_ac/accidents/part-m-00000 | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Import Accidents table from HDFS into Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "OK\n",
      "Time taken: 1.063 seconds\n"
     ]
    }
   ],
   "source": [
    "%%sh \n",
    "hive -e \"CREATE EXTERNAL TABLE if not exists db_bdtt_ac.accidents(Accident_Index varchar(50),Location_Easting_OSGR FLOAT,Location_Northing_OSGR FLOAT,Longitude DECIMAL(10,8),Latitude DECIMAL(10,8),Police_Force INT,Accident_Severity INT,Number_of_Vehicles INT,Number_of_Casualties INT,Date VARCHAR(50),Day_of_Week INT,Time VARCHAR(50),Local_Authority_District INT,Local_Authority_Highway VARCHAR(50), st_Road_Class INT,1st_Road_Number INT,Road_Type INT,Speed_limit FLOAT,Junction_Detail INT,Junction_Control INT,2nd_Road_Class INT,2nd_Road_Number INT,Pedestrian_Crossing_Human_Control INT,Pedestrian_Crossing_Physical_Facilities INT,Light_Conditions INT,Weather_Conditions INT,Road_Surface_Conditions INT,Special_Conditions_at_Site INT,Carriageway_Hazards INT,Urban_or_Rural_Area INT,Did_Police_Officer_Attend_Scene_of_Accident INT,LSOA_of_Accident_Location VARCHAR(50)) comment 'External table for accidents data' row format delimited fields terminated by ',' location '/db_bdtt_ac/accidents/' tblproperties(\\\"skip.header.line.count\\\"=\\\"1\\\")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017010009342\t526790.0\t181970.0\t-0.173845\t51.522425\t1\t3\t2\t1\t01/01/2017\t1\t01:30\t1\tE09000033\t3\t5\t6\t30.0\t3\t4\t6\t0\t0\t0\t4\t1\t2\t0\t0\t1\t1\tE01004702\n",
      "2017010009344\t535200.0\t181260.0\t-0.052969\t51.514096\t1\t3\t3\t1\t01/01/2017\t1\t00:30\t5\tE09000030\t3\t13\t6\t30.0\t3\t4\t5\t0\t0\t0\t4\t1\t1\t0\t0\t1\t1\tE01004298\n",
      "2017010009348\t534340.0\t193560.0\t-0.060658\t51.624832\t1\t3\t2\t1\t01/01/2017\t1\t01:11\t32\tE09000010\t3\t1010\t1\t30.0\t1\t4\t4\t154\t0\t4\t4\t2\t2\t0\t0\t1\t1\tE01001429\n",
      "2017010009350\t533680.0\t187820.0\t-0.072372\t51.573408\t1\t2\t1\t1\t01/01/2017\t1\t01:42\t4\tE09000012\t3\t107\t3\t20.0\t6\t2\t3\t10\t0\t5\t4\t1\t2\t0\t0\t1\t1\tE01001808\n",
      "2017010009351\t514510.0\t172370.0\t-0.353876\t51.438762\t1\t3\t2\t1\t01/01/2017\t1\t03:31\t24\tE09000027\t6\t0\t6\t30.0\t0\t-1\t-1\t0\t0\t0\t4\t1\t2\t0\t0\t1\t1\tE01003900\n",
      "2017010009353\t508640.0\t181870.0\t-0.435377\t51.525305\t1\t3\t2\t2\t01/01/2017\t1\t04:07\t26\tE09000017\t3\t4020\t3\t40.0\t3\t4\t5\t0\t0\t4\t4\t1\t2\t0\t0\t1\t1\tE01002401\n",
      "2017010009354\t527880.0\t181950.0\t-0.15815\t51.522\t1\t3\t2\t1\t01/01/2017\t1\t05:20\t1\tE09000033\t3\t501\t3\t30.0\t0\t-1\t-1\t0\t2\t5\t4\t2\t2\t4\t0\t1\t1\tE01004660\n",
      "2017010009357\t520940.0\t192820.0\t-0.254393\t51.621219\t1\t2\t1\t1\t01/01/2017\t1\t03:18\t30\tE09000003\t3\t1\t3\t50.0\t1\t2\t3\t5109\t0\t4\t4\t2\t2\t0\t0\t1\t1\tE01000264\n",
      "2017010009358\t531430.0\t178450.0\t-0.108314\t51.489732\t1\t2\t1\t1\t01/01/2017\t1\t03:00\t9\tE09000022\t3\t3204\t6\t30.0\t3\t4\t6\t0\t0\t0\t4\t1\t2\t0\t0\t1\t1\tE01003106\n",
      "2017010009360\t533910.0\t190610.0\t-0.067992\t51.598425\t1\t3\t2\t4\t01/01/2017\t1\t05:00\t31\tE09000014\t3\t1010\t6\t30.0\t6\t2\t4\t137\t0\t5\t4\t2\t2\t0\t0\t1\t1\tE01002077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "OK\n",
      "Time taken: 0.981 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "hive -e \"select * from db_bdtt_ac.accidents limit 10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Import casualties table from MySQL into Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/04/04 00:25:02 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5-cdh5.4.3\n",
      "19/04/04 00:25:02 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.\n",
      "19/04/04 00:25:02 INFO tool.BaseSqoopTool: Using Hive-specific delimiters for output. You can override\n",
      "19/04/04 00:25:02 INFO tool.BaseSqoopTool: delimiters with --fields-terminated-by, etc.\n",
      "19/04/04 00:25:02 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.\n",
      "19/04/04 00:25:02 INFO tool.CodeGenTool: Beginning code generation\n",
      "19/04/04 00:25:03 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `casualties` AS t LIMIT 1\n",
      "19/04/04 00:25:03 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `casualties` AS t LIMIT 1\n",
      "19/04/04 00:25:03 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/lib/hadoop-mapreduce\n",
      "Note: /tmp/sqoop-training/compile/ef7a3806099a7f7779be8bff6ff6a5a5/casualties.java uses or overrides a deprecated API.\n",
      "Note: Recompile with -Xlint:deprecation for details.\n",
      "19/04/04 00:25:06 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-training/compile/ef7a3806099a7f7779be8bff6ff6a5a5/casualties.jar\n",
      "19/04/04 00:25:06 WARN manager.MySQLManager: It looks like you are importing from mysql.\n",
      "19/04/04 00:25:06 WARN manager.MySQLManager: This transfer can be faster! Use the --direct\n",
      "19/04/04 00:25:06 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.\n",
      "19/04/04 00:25:06 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)\n",
      "19/04/04 00:25:06 INFO mapreduce.ImportJobBase: Beginning import of casualties\n",
      "19/04/04 00:25:06 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "19/04/04 00:25:06 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar\n",
      "19/04/04 00:25:07 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "19/04/04 00:25:07 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "19/04/04 00:25:08 INFO db.DBInputFormat: Using read commited transaction isolation\n",
      "19/04/04 00:25:08 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "19/04/04 00:25:09 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1553501649911_0031\n",
      "19/04/04 00:25:09 INFO impl.YarnClientImpl: Submitted application application_1553501649911_0031\n",
      "19/04/04 00:25:09 INFO mapreduce.Job: The url to track the job: http://localhost:8088/proxy/application_1553501649911_0031/\n",
      "19/04/04 00:25:09 INFO mapreduce.Job: Running job: job_1553501649911_0031\n",
      "19/04/04 00:25:17 INFO mapreduce.Job: Job job_1553501649911_0031 running in uber mode : false\n",
      "19/04/04 00:25:17 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "19/04/04 00:25:25 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "19/04/04 00:25:26 INFO mapreduce.Job: Job job_1553501649911_0031 completed successfully\n",
      "19/04/04 00:25:27 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=136782\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=87\n",
      "\t\tHDFS: Number of bytes written=7814703\n",
      "\t\tHDFS: Number of read operations=4\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tOther local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=6484\n",
      "\t\tTotal vcore-seconds taken by all map tasks=6484\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1659904\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=170993\n",
      "\t\tMap output records=170993\n",
      "\t\tInput split bytes=87\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=131\n",
      "\t\tCPU time spent (ms)=2030\n",
      "\t\tPhysical memory (bytes) snapshot=112541696\n",
      "\t\tVirtual memory (bytes) snapshot=845750272\n",
      "\t\tTotal committed heap usage (bytes)=47972352\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=7814703\n",
      "19/04/04 00:25:27 INFO mapreduce.ImportJobBase: Transferred 7.4527 MB in 19.6368 seconds (388.6346 KB/sec)\n",
      "19/04/04 00:25:27 INFO mapreduce.ImportJobBase: Retrieved 170993 records.\n",
      "19/04/04 00:25:27 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `casualties` AS t LIMIT 1\n",
      "19/04/04 00:25:27 INFO hive.HiveImport: Loading uploaded data into Hive\n",
      "\n",
      "Logging initialized using configuration in jar:file:/usr/lib/hive/lib/hive-common-1.1.0-cdh5.4.3.jar!/hive-log4j.properties\n",
      "OK\n",
      "Time taken: 2.408 seconds\n",
      "Loading data to table db_bdtt_ac.casualties\n",
      "chgrp: changing ownership of 'hdfs://localhost:8020/user/hive/warehouse/db_bdtt_ac.db/casualties/part-m-00000': User does not belong to hive\n",
      "Table db_bdtt_ac.casualties stats: [numFiles=1, totalSize=7814703]\n",
      "OK\n",
      "Time taken: 0.549 seconds\n"
     ]
    }
   ],
   "source": [
    "! sqoop --options-file ./database.props \\\n",
    "        --table casualties \\\n",
    "        --split-by 'Accident_Index'\\\n",
    "        --hive-import \\\n",
    "        --hive-database db_bdtt_ac \\\n",
    "        --hive-table casualties \\\n",
    "        --null-string '-1' \\\n",
    "        --null-non-string '-1' \\\n",
    "        --m 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017010001708\t1\t1\t2\t2\t18\t4\t3\t0\t0\t1\t0\t0\t9\t1\t2\n",
      "2017010001708\t2\t2\t1\t1\t19\t4\t2\t0\t0\t0\t0\t0\t2\t-1\t-1\n",
      "2017010001708\t2\t3\t2\t1\t18\t4\t1\t0\t0\t0\t0\t0\t2\t-1\t-1\n",
      "2017010009342\t1\t1\t2\t2\t33\t6\t3\t0\t0\t1\t0\t0\t9\t1\t5\n",
      "2017010009344\t3\t1\t1\t2\t31\t6\t3\t0\t0\t0\t0\t0\t9\t1\t5\n",
      "2017010009348\t1\t1\t2\t1\t3\t1\t3\t0\t0\t2\t0\t0\t9\t1\t2\n",
      "2017010009350\t1\t1\t3\t1\t45\t7\t2\t4\t3\t0\t0\t2\t0\t-1\t-1\n",
      "2017010009351\t2\t1\t1\t1\t14\t3\t3\t0\t0\t0\t0\t0\t3\t1\t9\n",
      "2017010009353\t1\t1\t1\t2\t58\t9\t3\t0\t0\t0\t0\t0\t9\t1\t2\n",
      "2017010009353\t2\t2\t1\t1\t27\t6\t3\t0\t0\t0\t0\t0\t9\t1\t3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "OK\n",
      "Time taken: 0.902 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "hive -e \"select * from db_bdtt_ac.casualties limit 10\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Import casualties table from MySQL into Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "OK\n",
      "Time taken: 0.872 seconds\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "hive -e \"CREATE EXTERNAL TABLE if not exists  db_bdtt_ac.vehicles(Accident_Index VARCHAR(50),Vehicle_Reference INT,Vehicle_Type INT,Towing_and_Articulation INT,Vehicle_Manoeuvre INT,Vehicle_Location_Restricted_Lane INT,Junction_Location INT,Skidding_and_Overturning INT,Hit_Object_in_Carriageway INT,Vehicle_Leaving_Carriageway INT,Hit_Object_off_Carriageway INT,1st_Point_of_Impact INT,Was_Vehicle_Left_Hand_Drive INT,Journey_Purpose_of_Driver INT,Sex_of_Driver INT,Age_of_Driver INT,Age_Band_of_Driver INT,Engine_Capacity_CC INT,Propulsion_Code INT,Age_of_Vehicle INT,Driver_IMD_Decile INT,Driver_Home_Area_Type INT,Vehicle_IMD_Decile INT) comment 'External table for vehicles data' row format delimited fields terminated by ',' location '/db_bdtt_ac/vehicles/' tblproperties(\\\"skip.header.line.count\\\"=\\\"1\\\");\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017010001708\t1\t9\t0\t18\t0\t0\t0\t0\t0\t0\t1\t1\t6\t1\t24\t5\t1997\t2\t1\t-1\t-1\t-1\n",
      "2017010001708\t2\t2\t0\t18\t0\t0\t1\t0\t0\t0\t2\t1\t6\t1\t19\t4\t-1\t-1\t-1\t-1\t-1\t-1\n",
      "2017010009342\t1\t9\t0\t18\t0\t1\t0\t0\t0\t0\t2\t1\t6\t1\t33\t6\t1797\t8\t8\t9\t1\t9\n",
      "2017010009342\t2\t9\t0\t18\t0\t1\t1\t0\t0\t0\t1\t1\t6\t1\t40\t7\t2204\t2\t12\t2\t1\t2\n",
      "2017010009344\t1\t9\t0\t18\t0\t1\t0\t0\t0\t0\t1\t1\t6\t3\t-1\t-1\t-1\t-1\t-1\t-1\t-1\t-1\n",
      "2017010009344\t2\t9\t0\t18\t0\t2\t0\t0\t0\t0\t1\t1\t6\t1\t35\t6\t1896\t2\t9\t-1\t-1\t-1\n",
      "2017010009344\t3\t9\t0\t18\t0\t2\t0\t0\t0\t0\t1\t1\t6\t2\t31\t6\t1299\t1\t14\t5\t1\t5\n",
      "2017010009348\t1\t9\t0\t18\t0\t4\t0\t9\t0\t0\t1\t1\t6\t2\t37\t7\t1399\t1\t9\t2\t1\t2\n",
      "2017010009348\t2\t9\t0\t9\t0\t3\t0\t0\t0\t0\t3\t1\t6\t2\t29\t6\t1797\t8\t6\t4\t1\t4\n",
      "2017010009350\t1\t9\t0\t18\t0\t2\t0\t0\t0\t0\t3\t1\t6\t1\t78\t11\t1988\t1\t14\t4\t1\t4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Logging initialized using configuration in file:/etc/hive/conf.dist/hive-log4j.properties\n",
      "OK\n",
      "Time taken: 1.686 seconds, Fetched: 10 row(s)\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "hive -e \"select * from db_bdtt_ac.vehicles limit 10\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
